{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "from IPython.utils import io\n",
    "import sys\n",
    "warnings.filterwarnings('ignore')\n",
    "#pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load filtered HCRIS PUF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "puf_df = pd.read_pickle('~/GitHub/HCRIS-databuilder/Filtered_PUF_data/FilteredEngineeredPUF_p5.pkl')\n",
    "puf_df = puf_df.applymap(lambda x: x.decode() if isinstance(x, bytes) else x)\n",
    "puf_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Load SAS database files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with io.capture_output() as captured: main_df = pd.read_sas('~/Desktop/HCRIS/hosp10-sas/prds_hosp10_yr2010.sas7bdat')\n",
    "main_df['File Date'] = ['2010'] * main_df.shape[0]\n",
    "main_df = main_df.applymap(lambda x: x.decode() if isinstance(x, bytes) else x)\n",
    "main_df['rpt_rec_num'] = main_df['rpt_rec_num'].astype(int)\n",
    "main_df['rpt_rec_num'] = main_df['rpt_rec_num'].astype(str)\n",
    "print('2010: (rows, columns) =', main_df.shape)\n",
    "\n",
    "yrs = ['2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022']\n",
    "for yr in yrs:\n",
    "    with io.capture_output() as captured: tdf = pd.read_sas('~/Desktop/HCRIS/hosp10-sas/prds_hosp10_yr' + yr + '.sas7bdat')\n",
    "    tdf['File Date'] = [yr] * tdf.shape[0]\n",
    "    tdf = tdf.applymap(lambda x: x.decode() if isinstance(x, bytes) else x)\n",
    "    tdf['rpt_rec_num'] = tdf['rpt_rec_num'].astype(int)\n",
    "    tdf['rpt_rec_num'] = tdf['rpt_rec_num'].astype(str)\n",
    "\n",
    "    main_df = pd.concat([main_df, tdf], ignore_index=True)\n",
    "    print(yr + ': (rows, columns) =', main_df.shape)\n",
    "\n",
    "del tdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_replace = ['rpt_rec_num', 'prvdr_num', 'fi_num', 'rpt_stus_cd', 'fi_creat_dt', \n",
    "              'fy_bgn_dt', 'fy_end_dt', 'util_cd', 'trnsmtl_num', 'state', \n",
    "              'st_cty_cd', 'census', 'region', 'proc_dt', 'msa', 'H3_1_HHA1_C10_15____',\n",
    "              'H3_1_HHA1_C11_15____', 'sub3', \n",
    "             ]\n",
    "\n",
    "replacement = ['RPT_REC_NUM', 'PRVDR_NUM', 'FI_NUM', 'RPT_STUS_CD', 'FI_CREAT_DT', \n",
    "               'FY_BGN_DT', 'FY_END_DT', 'UTIL_CODE', 'TRNSMTL_NUM', 'STATE', \n",
    "               'ST_CTY_CD', 'CENSUS', 'REGION', 'PROC_DT', 'MSA', 'H3_1_HHA1_C10_15',\n",
    "               'H3_1_HHA1_C11_15', 'SUB3',\n",
    "              ]\n",
    "\n",
    "main_df.rename(columns = {to_replace[0]: replacement[0], to_replace[1]: replacement[1],\n",
    "                           to_replace[2]: replacement[2], to_replace[3]: replacement[3],\n",
    "                           to_replace[4]: replacement[4], to_replace[5]: replacement[5],\n",
    "                           to_replace[6]: replacement[6], to_replace[7]: replacement[7],\n",
    "                           to_replace[8]: replacement[8], to_replace[9]: replacement[9],\n",
    "                           to_replace[10]: replacement[10], to_replace[11]: replacement[11],\n",
    "                           to_replace[12]: replacement[12], to_replace[13]: replacement[13],\n",
    "                           to_replace[14]: replacement[14], to_replace[15]: replacement[15],\n",
    "                           to_replace[16]: replacement[16], to_replace[17]: replacement[17],\n",
    "                         }, inplace = True)\n",
    "\n",
    "main_df.drop(labels=['_NAME_', 'E_A_HOS_C1_68', 'E_A_HOS_C1_7090', 'E_A_HOS_C1_7091', 'E_A_HOS_C1_7093', \n",
    "                     'E_A_HOS_C1_7096', 'E_A_HOS_C1_93', 'E_A_HOS_C1_47', 'E_A_HOS_C1_49', 'E_A_HOS_C1_50', \n",
    "                     'E_A_HOS_C1_54', 'E_A_HOS_C1_59', 'E_A_HOS_C1_7094', 'E_A_HOS_C1_72', 'E_A_HOS_C1_7099', \n",
    "                     'E_A_HOS_C1_7097', 'E_A_HOS_C1_48', 'S2_1_C1_35', 'S2_1_C2_2'], axis=1, inplace=True)\n",
    "\n",
    "print('main_df.shape:', main_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_features = list(filter(lambda x:x in list(puf_df), list(main_df)))\n",
    "print(common_features)\n",
    "\n",
    "main_df['RPT_REC_NUM'] = main_df['RPT_REC_NUM'].astype(int)\n",
    "main_df['RPT_REC_NUM'] = main_df['RPT_REC_NUM'].astype(str)\n",
    "main_df = main_df.merge(puf_df, how='outer', on=common_features)\n",
    "del puf_df\n",
    "\n",
    "main_df.sort_values(by='Reconstructed HAC penalty', inplace=True, ascending=False)\n",
    "\n",
    "print(main_df.shape)\n",
    "main_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "crosswalk_df = pd.read_csv('~/GitHub/HCRIS-databuilder/crosswalk/2552-10 SAS FILE RECORD LAYOUT AND CROSSWALK TO 96 - 2021.csv', sep=',')\n",
    "crosswalk_labels = crosswalk_df['10_FIELD_NAME'].tolist()\n",
    "crosswalk_labels = [str(x).strip(' ') for x in crosswalk_labels]\n",
    "crosswalk_df['10_FIELD_NAME'] = crosswalk_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "main_df_col_labels = list(main_df)\n",
    "main_df_col_labels = [str(x).strip(' ') for x in main_df_col_labels]\n",
    "\n",
    "print('\\nNumber of labels in the crosswalk:', len(crosswalk_labels))\n",
    "print('Number of unique labels in the crosswalk:', len(list(set(crosswalk_labels))))\n",
    "\n",
    "print('\\nNumber of labels in the main_df:', len(main_df_col_labels))\n",
    "print('Number of unique labels in the main_df:', len(list(set(main_df_col_labels))))\n",
    "\n",
    "shared_labels = list(set(main_df_col_labels) & set(crosswalk_labels))\n",
    "print('\\nNumber of labels shared between the main dataframe and the crosswalk:', len(shared_labels))\n",
    "shared_labels.append('File Date')\n",
    "\n",
    "dif = set(main_df_col_labels).difference(crosswalk_labels)\n",
    "print('\\n' + str(len(dif)) + ' labels in main dataframe but not in crosswalk:')\n",
    "print(dif)\n",
    "\n",
    "dif = set(crosswalk_labels).difference(main_df_col_labels)\n",
    "print('\\n' + str(len(dif)) + ' labels in crosswalk but not in main dataframe:')\n",
    "print(dif)\n",
    "\n",
    "del main_df_col_labels\n",
    "del dif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_labels = ['Line 19 Subtotal', \n",
    "              'Reconstructed HAC penalty', \n",
    "              'HAC penalty imputed from E_A_HOS_C1_59',\n",
    "              'Reconstructed IPPS payment (post HAC penalty)', \n",
    "              'Reconstructed IPPS payment (pre HAC penalty)',\n",
    "              ]\n",
    "\n",
    "main_df = main_df[main_df.columns[main_df.columns.isin(shared_labels + add_labels)]]\n",
    "del shared_labels\n",
    "\n",
    "main_df = main_df.dropna(axis=1, how='all')\n",
    "main_df = main_df.dropna(axis=0, how='all')\n",
    "print('main_df.shape (columns and rows with no data removed):', main_df.shape)\n",
    "\n",
    "CODE = []\n",
    "FIELD_DESCRIPTION = []\n",
    "TYPE = []\n",
    "SUBTYPE = []\n",
    "\n",
    "col_labels = list(main_df)\n",
    "\n",
    "for lab in col_labels:\n",
    "    if lab == 'File Date':\n",
    "        CODE.append('File Date')\n",
    "        FIELD_DESCRIPTION.append('File Date')\n",
    "        TYPE.append('File Date')\n",
    "        SUBTYPE.append('File Date')\n",
    "        \n",
    "        \n",
    "    elif lab in add_labels:\n",
    "        CODE.append('')\n",
    "        FIELD_DESCRIPTION.append(lab)\n",
    "        TYPE.append('CALCULATION OF REIMBURSEMENT SETTLEMENT (PPS)')\n",
    "        SUBTYPE.append('')\n",
    "        \n",
    "    else:\n",
    "        df_sub = crosswalk_df[crosswalk_df['10_FIELD_NAME'] == lab]\n",
    "        CODE.append(df_sub['10_FIELD_NAME'].iloc[0])\n",
    "        \n",
    "        x = df_sub['FIELD DESCRIPTION '].iloc[0]\n",
    "        if x == \"\" or pd.isnull(x):\n",
    "            FIELD_DESCRIPTION.append('No Description')\n",
    "            \n",
    "        else:\n",
    "            FIELD_DESCRIPTION.append(x)\n",
    "        \n",
    "        x = df_sub['TYPE'].iloc[0]\n",
    "        if x == \"\" or pd.isnull(x):\n",
    "            TYPE.append('No Description')\n",
    "        else:\n",
    "            TYPE.append(x)\n",
    "        \n",
    "        x = df_sub['SUBTYPE'].iloc[0]\n",
    "        if x == \"\" or pd.isnull(x):\n",
    "                SUBTYPE.append('')\n",
    "        else:\n",
    "            SUBTYPE.append(x)\n",
    "\n",
    "SUBTYPE = pd.Series(SUBTYPE).fillna('').tolist()\n",
    "TYPE = pd.Series(TYPE).fillna('').tolist()\n",
    "\n",
    "for i, val in enumerate(SUBTYPE):\n",
    "    if val == 'File Date':\n",
    "        continue\n",
    "    elif val == '' and CODE[i] == '':\n",
    "        SUBTYPE[i] = str(FIELD_DESCRIPTION[i])\n",
    "    elif val == '':\n",
    "        SUBTYPE[i] = str(FIELD_DESCRIPTION[i]) + ' ' + '(' + str(CODE[i]) + ')'\n",
    "    else:\n",
    "        SUBTYPE[i] = str(val) + ' ' + str(FIELD_DESCRIPTION[i]) + ' (' + str(CODE[i]) + ')'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame([col_labels, FIELD_DESCRIPTION, TYPE, SUBTYPE], columns=col_labels)\n",
    "main_df = pd.concat([df2, main_df])\n",
    "del df2\n",
    "\n",
    "main_df.columns = pd.MultiIndex.from_arrays(main_df.iloc[0:4].values)\n",
    "main_df = main_df.iloc[4:]\n",
    "\n",
    "del col_labels\n",
    "del FIELD_DESCRIPTION\n",
    "del TYPE\n",
    "del SUBTYPE\n",
    "\n",
    "main_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CMS_Gen_Info_df = pd.read_csv('~/GitHub/HCRIS-databuilder/GeoData/Hospital_General_Information.tsv', sep='\\t')\n",
    "print(list(CMS_Gen_Info_df))\n",
    "CMS_Gen_Info_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_ls = main_df.iloc[:, (main_df.columns.get_level_values(0) == 'S2_1_C1_3')].T.values.tolist()[0]\n",
    "id_ls = main_df.iloc[:, (main_df.columns.get_level_values(0) == 'PRVDR_NUM')].T.values.tolist()[0]\n",
    "\n",
    "print(hospital_ls[0:4])\n",
    "print(id_ls[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lats = []\n",
    "lons = []\n",
    "Htypes = []\n",
    "Ctypes = []\n",
    "\n",
    "misses = 0\n",
    "hits = 0\n",
    "for i, h in enumerate(hospital_ls):\n",
    "    \n",
    "    fid = id_ls[i]\n",
    "    \n",
    "    try:\n",
    "        df = CMS_Gen_Info_df[CMS_Gen_Info_df['Facility ID'] == fid]\n",
    "            \n",
    "        loc = df['Location'].iloc[0]\n",
    "        loc = loc.replace(\"POINT (\",\"\") \n",
    "        loc = loc.replace(\")\",\"\")\n",
    "        loc = loc.split(\" \")\n",
    "            \n",
    "        lat = loc[1]\n",
    "        lon = loc[0]\n",
    "        lats.append(lat)\n",
    "        lons.append(lon)\n",
    "        \n",
    "        htype = df['Hospital Type'].iloc[0]\n",
    "        Htypes.append(htype)\n",
    "        ctype = df['Hospital Ownership'].iloc[0]\n",
    "        Ctypes.append(ctype)\n",
    "        \n",
    "        del df\n",
    "        \n",
    "    except:\n",
    "        lats.append(float('NaN'))\n",
    "        lons.append(float('NaN'))\n",
    "        Htypes.append(float('NaN'))\n",
    "        Ctypes.append(float('NaN'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, val in enumerate(lats):\n",
    "    if pd.isnull(val): \n",
    "        continue\n",
    "    else:\n",
    "        print(val, lons[i], Htypes[i], Ctypes[i])\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df[('Lat', 'Lat', 'Lat', 'Lat')] = lats\n",
    "main_df[('Lon', 'Lon', 'Lon', 'Lon')] = lons\n",
    "main_df[('Control type, text', 'Control type, text', 'Control type, text', 'Control type, text')] = Ctypes\n",
    "main_df[('Hospital type, text', 'Hospital type, text', 'Hospital type, text', 'Hospital type, text')] = Htypes\n",
    "main_df[('Num and Name', 'Num and Name', 'Num and Name', 'Num and Name')] = main_df[('S2_1_C1_3', 'Hospital Name ', 'No Description', 'Hospital Name  (S2_1_C1_3)')].astype(str) +' (' + main_df[('PRVDR_NUM', 'Hospital Provider Number ', 'HOSPITAL IDENTIFICATION INFORMATION', 'Hospital Provider Number  (PRVDR_NUM)')].astype(str) + ')'   \n",
    "\n",
    "del lats\n",
    "del lons\n",
    "del Ctypes\n",
    "del Htypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "badnames = ['20994', float('NaN'), '-0007', '1', '330354', '4499 ACUSHNET AVENUE OPERATING COMPA',\n",
    "           '4499 ACUSHNET AVENUE OPERATING COMPM']\n",
    "\n",
    "main_df = main_df[~main_df[('S2_1_C1_3', 'Hospital Name ', 'No Description', 'Hospital Name  (S2_1_C1_3)')].isin(badnames)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = main_df.filter(items=[('Lat', 'Lat', 'Lat', 'Lat'),\n",
    "                             ('Lon', 'Lon', 'Lon', 'Lon'),\n",
    "                             ('Control type, text', 'Control type, text', 'Control type, text', 'Control type, text'),\n",
    "                             ('Hospital type, text', 'Hospital type, text', 'Hospital type, text', 'Hospital type, text'),\n",
    "                             ('Num and Name', 'Num and Name', 'Num and Name', 'Num and Name'),\n",
    "                             ('S3_1_C2_27', 'Total Facility', 'NUMBER OF BEDS', 'Total Facility (S3_1_C2_27)'),\n",
    "                             ('S2_1_C2_2', 'Hospital State', 'No Description', 'Hospital State (S2_1_C2_2)'),\n",
    "                            ], axis=1)\n",
    "\n",
    "tdf.to_pickle('GenDat4App/GenDat4App_p4.pkl', protocol=4)\n",
    "\n",
    "del tdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df[('FY_END_DT', 'Fiscal Year End Date ', 'HOSPITAL IDENTIFICATION INFORMATION', 'Fiscal Year End Date  (FY_END_DT)')] = pd.to_datetime(main_df[('FY_END_DT', 'Fiscal Year End Date ', 'HOSPITAL IDENTIFICATION INFORMATION', 'Fiscal Year End Date  (FY_END_DT)')])\n",
    "main_df = main_df.sort_values(by=[('Num and Name', 'Num and Name', 'Num and Name', 'Num and Name'),\n",
    "                     ('FY_END_DT', 'Fiscal Year End Date ', 'HOSPITAL IDENTIFICATION INFORMATION', 'Fiscal Year End Date  (FY_END_DT)')],\n",
    "                     ascending=[True, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "numNname = sorted(list(set(main_df[('Num and Name', 'Num and Name', 'Num and Name', 'Num and Name')].tolist())))\n",
    "\n",
    "for i, val in enumerate(numNname):\n",
    "    prvdr = re.sub('\\ |\\?|\\.|\\!|\\/|\\;|\\:', '', val)\n",
    "\n",
    "    tdf = main_df[main_df[('Num and Name', 'Num and Name', 'Num and Name', 'Num and Name')] == val]\n",
    "    tdf.to_csv('provider_data/' + prvdr + '.csv')\n",
    "    del tdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.to_pickle('hcris_all_data/HCRIS_p4.pkl', protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "report_categories = list(set(main_df.columns.get_level_values(2).tolist()))\n",
    "report_categories = [x for x in report_categories if str(x) != 'nan']\n",
    "report_categories = [x for x in report_categories if str(x) != 'Lat']\n",
    "report_categories = [x for x in report_categories if str(x) != 'Lon']\n",
    "report_categories = [x for x in report_categories if str(x) != 'Num and Name']\n",
    "report_categories = [x for x in report_categories if str(x) != 'Hospital type, text']\n",
    "#report_categories = [x for x in report_categories if str(x) != '']\n",
    "report_categories = [x for x in report_categories if str(x) != 'HOSPITAL IDENTIFICATION INFORMATION']\n",
    "\n",
    "report_categories = [x for x in report_categories if str(x) != 'Control type, text']\n",
    "report_categories = [x for x in report_categories if str(x) != 'HOSPITAL IDENTIFICATION INFORMATION']\n",
    "report_categories = [x for x in report_categories if str(x) != 'HOSPITAL IDENTIFICATION INFORMATION']\n",
    "report_categories.sort()\n",
    "\n",
    "import csv\n",
    "\n",
    "with open('GenDat4App/report_categories.csv', 'w+', newline='') as OUT:\n",
    "    writer = csv.writer(OUT)\n",
    "    writer.writerow(report_categories)\n",
    "    \n",
    "    \n",
    "sub_categories = list(set(main_df.columns.get_level_values(3).tolist()))\n",
    "sub_categories.sort()\n",
    "\n",
    "with open('GenDat4App/sub_categories.csv', 'w+', newline='') as OUT:\n",
    "    writer = csv.writer(OUT)\n",
    "    writer.writerow(sub_categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
